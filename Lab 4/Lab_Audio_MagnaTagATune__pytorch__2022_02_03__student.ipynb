{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ROhVsdwXLtO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u63kmX24XOwq"
   },
   "source": [
    "# Lab Audio/ Music Auto-Tagging multi-label\n",
    "\n",
    "**WARNING: This jupiter notebook was created as part of Télécom-Paris teaching programme and is distributed only to its students. \n",
    "Any re-use, modification, distribution outside this framework or making it available (through github, colab or others) is forbidden.**\n",
    "\n",
    "- Date: 2022/02/07\n",
    "- Version: v0.1\n",
    "- Author: geoffroy.peeters@telecom-paris.fr\n",
    "\n",
    "## Objectives of this Lab:\n",
    "\n",
    "[Magna-Tag-A-Tune](https://musicmachinery.com/2009/04/01/magnatagatune-a-new-research-data-set-for-mir/) is a dataset containing 25,863 music clips belonging to one of the 5223 songs, 445 albums and 230 artists. \n",
    "The clips span a broad range of genres like Classical, New Age, Electronica, Rock, Pop, World, Jazz, Blues, Metal, Punk, and more. \n",
    "Each audio clip has been annotated by humans playing the two-player online Tag-A-Tune game. \n",
    "The annotations include tags like ’singer’, ’no singer’, ’violin’, ’drums’, ’classical’, ’jazz’. \n",
    "\n",
    "From this large dataset, we extracted a smaller dataset (1000 music clips) corresponding to the annotation into **instrumentation**.\n",
    "The labels of this dataset are highly unbalanced (some tags are much frequent than others).\n",
    "In this lab, you will develop a system to perform the automatic estimation of these tags. Since tags can co-occur (they are not mutually exclusive), the problem is a **multi-label classification**.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "This dataset is available as a .json file and a .zip file.\n",
    "You will first upload both to your Google Drive in a folder named 'My Drive/_sound/_magnatagatune'.\n",
    "You will first **mount your Google drive** in this notebook to allow accessing its storage.\n",
    "For this lab, we will also run the code on the GPU of Google colab. Do not forget to set **execution type** to GPU.\n",
    "\n",
    "## Feature extraction.\n",
    "For each file, you will extract its audio representations/features using ```librosa```.\n",
    "The representation we will use is the Log-Mel-Spectrogram (with 128 Mel bands), i.e. the Mel-Spectrogram in Log-scale. \n",
    "This representation is the standard inut representation for Deep Neural Network; it is close to the MFCC one but skipping the last DCT.\n",
    "Note that we will use a $\\log(1 + C x)$ with $C=10000$ instead of the standard $\\log(x)$ function. This is to avoid over-representing very small values.\n",
    "\n",
    "In order to avoid having to compute the audio representations/features each time we need them, we will store them as files in the Drive.\n",
    "\n",
    "## Audio patches\n",
    "We will then use a ConvNet to perform the classification of the file over time. \n",
    "For this the big matrix containing the audio representations./features is sliced over time as a succession of 2D patches.\n",
    "These 2D patches are the inputs of our ```ConvNet```. \n",
    "A single file is therefore represented by a succession of 2D patches.\n",
    "\n",
    "## Dataset and Dataloader \n",
    "\n",
    "Unlike scholar examples (using make_circle or MNIST) it is not possible to get all the data simultaneously in memory and then select the one needed for each mini-batch. Instead, we need to develop a ```generator```which will provide the data on-demand (get in memory only the data needed to process a mini-batch). We will do this using the very very convenient ```DataSet``` and ```DataLoader``` from pytorch.\n",
    "\n",
    "## Deep Neural Network\n",
    "\n",
    "We then train a ConvNet which takes as input the patches (by using the DataLoader) and return the probability of each tag. \n",
    "\n",
    "The network has as many outputs neurons as the number of tags.\n",
    "Since we deal multi-label, neurons are independent of each others. \n",
    "We then use a sigmoid for each.\n",
    "We minimize the sum of the Binary Cross Entropy (BCE) over neurons.\n",
    "Note that in pytorch it is common to combine the sigmoid and the BCE into a new loss; here denoted ``BCEWithLogitsLoss``. \n",
    "In this case, the output of the network does not have any non-linear functions. \n",
    "In multi-label classification, for each class $c$, the number of negative examples $|y^{(i)}_c=0|$ is usually much larger than of positive examples $|y^{(i)}_c=1|$. It is therefore benefitial to give a higher weights to positive examples in ``BCEWithLogitsLoss``.\n",
    "\n",
    "## Testing: tag-o-gram\n",
    "\n",
    "Once train, we will apply our model to compute the tag-o-gram (likelihood of each tags over time) of an unknown file.\n",
    "\n",
    "\n",
    "\n",
    "## At the end of this Lab, the student will be able to\n",
    "- Use colab and connect it to Google Drive to work on/store remote data\n",
    "- Process datasets directly from Colab\n",
    "- Use librosa https://librosa.github.io/librosa/ to compute audio representations \n",
    "- Use DataSet and DataLoaders to get on the fly the necessary data for a batch\n",
    "- Perform a multi-label task (tag an audio file into multi-labels) using a Deep Convolutional Neural Network in pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iF7Upav4jejH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import pprint as pp\n",
    "\n",
    "import librosa\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1Pp7UZYmbDU"
   },
   "outputs": [],
   "source": [
    "# If necessary, install ffmpeg to allows mp3 decoding\n",
    "#! pip install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IbWgRqNT3YiR"
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UP_O4pYEjejK"
   },
   "outputs": [],
   "source": [
    "use_colab = True\n",
    "use_gpu = True\n",
    "\n",
    "do_magnatagatune1_gtzan2  = 1\n",
    "\n",
    "patch_hop_frame = 17\n",
    "patch_halfduration_frame = 34\n",
    "data_ext = '.lms'\n",
    "do_norm = True\n",
    "\n",
    "dropout_prob = 0.2\n",
    "nb_epoch = 50\n",
    "do_model_train = True\n",
    "\n",
    "do_student = True\n",
    "\n",
    "do_class_method = 'multilabel'\n",
    "subDIR = '/_magnatagatune/'\n",
    "dataset_json = 'dataset_magna-tag-a-tune-sub-instr-100.json'\n",
    "dataset_zip = 'dataset_magna-tag-a-tune.zip'\n",
    "dataset_subDIR =  '/dataset_magna-tag-a-tune/'\n",
    "labelname_dict_l = ['guitar', 'string', 'synth', 'drum', 'violin', 'piano', 'female', 'male', 'sitar', 'flute', 'harpsichord', 'harp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGnCZ2RdjejL"
   },
   "source": [
    "# Set Google Drive\n",
    "\n",
    "When you store locally data in Colab; these data will be removed at the end of your session.\n",
    "In order to able to store definitely your data, you can connect Colab to your Google Drive and then store the data on it. It is done in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rKZ6cRvtjejM",
    "outputId": "8c74a0d5-cdef-4feb-cd2f-7ec5b94edc24"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "mnt_point = '/content/drive/'\n",
    "drive.mount(mnt_point)\n",
    "DIR = mnt_point + '/My Drive/_sound/' + subDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu-JhRVT0s5I"
   },
   "source": [
    "We now unzip the file containing the audio of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5-a5C3akRoj",
    "outputId": "a780ec76-0017-4271-e9b3-4d3393d418b2"
   },
   "outputs": [],
   "source": [
    "shutil.unpack_archive(DIR + dataset_zip, DIR)\n",
    "len(glob.glob(DIR + dataset_subDIR + '*/*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbCpAts5jejN"
   },
   "source": [
    "# Load and prepare dataset\n",
    "\n",
    "The dataset (relationships between audio files and tags) is described in a json file.\n",
    "We load this file and convert the tags to a multi-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Pgnh-bjjejO"
   },
   "outputs": [],
   "source": [
    "with open(DIR + dataset_json, 'r') as f:\n",
    "    dataset_l = json.load(f)\n",
    "\n",
    "if do_class_method=='multilabel':\n",
    "  # ---- Convert tag to index of class\n",
    "  for data in dataset_l:\n",
    "      data['target'] = np.zeros(len(labelname_dict_l))\n",
    "      for tag in data['tag']:\n",
    "          if tag in labelname_dict_l:\n",
    "              data['target'][ labelname_dict_l.index(tag) ] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qbw3VvcHd3Gg"
   },
   "source": [
    "### Test\n",
    "\n",
    "You should get the following output.\n",
    "```\n",
    "len(dataset_l):  1000\n",
    "labelname_dict_l:  ['guitar', 'string', 'synth', 'drum', 'violin', 'piano', 'female', 'male', 'sitar', 'flute', 'harpsichord', 'harp']\n",
    "{'album': 'J.S. Bach Solo Cantatas',\n",
    " 'artist': 'American Bach Soloists',\n",
    " 'clip_id': 6,\n",
    " 'mp3_path': 'f/american_bach_soloists-j_s__bach_solo_cantatas-01-bwv54__i_aria-146-175.mp3',\n",
    " 'original_url': 'http://he3.magnatune.com/all/01--BWV54%20-%20I%20Aria--ABS.mp3',\n",
    " 'segmentEnd': 175,\n",
    " 'segmentStart': 146,\n",
    " 'tag': ['classical',\n",
    "         'violin',\n",
    "         'string',\n",
    "         'classical',\n",
    "         'opera',\n",
    "         'violin',\n",
    "         'baroque'],\n",
    " 'target': array([0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]),\n",
    " 'title': 'BWV54 - I Aria',\n",
    " 'track_number': 1,\n",
    " 'url': 'http://www.magnatune.com/artists/albums/abs-solocantatas/'}\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4D5Nx1Nd4-Y",
    "outputId": "a9d35fb3-5c17-4740-e71e-62acbaf28c1d"
   },
   "outputs": [],
   "source": [
    "print('len(dataset_l): ', len(dataset_l))\n",
    "print('labelname_dict_l: ',  labelname_dict_l )\n",
    "pp.pprint(dataset_l[1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IBa1z_ojejQ"
   },
   "source": [
    "# Audio features\n",
    "\n",
    "We create here a function that, given an audio file as input, \n",
    "- load the audio file\n",
    "- extract a given audio representation/ audio features\n",
    "- store the results in the drive; we do so since if this only needs to be computed once. The second time we call this function on the same file, it will load the results from the drive instead of recomputing it.\n",
    "\n",
    "We will use the ```librosa``` package for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M3-RYkfjejQ"
   },
   "outputs": [],
   "source": [
    "def F_do_standardization(data_m):\n",
    "    \"\"\"\n",
    "    description:\n",
    "      perform feature standardization (mean, std) over frames\n",
    "    inputs:\n",
    "      - data_m (nb_dim, nb_frame)\n",
    "    outputs:\n",
    "      - data_m (nb_dim, nb_frame)\n",
    "    \"\"\"\n",
    "    if do_student:\n",
    "        # --- START CODE HERE\n",
    "        ...\n",
    "        # --- STOP CODE HERE\n",
    "          \n",
    "    return data_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARXgIDW7cuSM"
   },
   "outputs": [],
   "source": [
    "def F_log(data_m, C = 10000):\n",
    "  \"\"\"\n",
    "  description:\n",
    "    replacement to log function (to avoid emphasazing very small values)\n",
    "  \"\"\"\n",
    "  return np.log(1 + C*data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yiKooZzJjejR"
   },
   "outputs": [],
   "source": [
    "def F_get_audio_features(audio_filename, data_ext=''):\n",
    "    \"\"\"\n",
    "    description:\n",
    "      compute Log-Mel-Sepctrogram audio features with n_mels=128, window length L_n = 2048, and hop-size STEP_n = 1024\n",
    "    inputs:\n",
    "      - audio_filename\n",
    "      - data_ext\n",
    "    outputs:\n",
    "      - data_m (nb_dim, nb_frame): Log-Mel-Sepctrogram matrix\n",
    "      - time_sec_v (nb_frame): corresponding time [in sec] of analysis windows\n",
    "    \"\"\"\n",
    "    L_n = 2048\n",
    "    STEP_n = 1024\n",
    "    feature_filename = audio_filename + data_ext + '.npz'\n",
    "\n",
    "    if not os.path.isfile(feature_filename):\n",
    "        \n",
    "        if do_student:\n",
    "            # --- START CODE HERE\n",
    "            ...\n",
    "            data_m = ...\n",
    "            time_sec_v = ...\n",
    "            # --- STOP CODE HERE\n",
    "\n",
    "        np.savez(feature_filename, data_m=data_m, time_sec_v=time_sec_v)\n",
    "    else:\n",
    "        all = np.load(feature_filename)\n",
    "        data_m = all['data_m']\n",
    "        time_sec_v = all['time_sec_v']\n",
    "\n",
    "    return data_m, time_sec_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJi7qM7sdxzR"
   },
   "source": [
    "### Test\n",
    "\n",
    "We test the extraction of audio features on a single audio file. You should get the following output.\n",
    "\n",
    "```\n",
    "shape: (128, 628), min: 0.0, max: 9.236710548400879\n",
    "```\n",
    "\n",
    "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_MagnaTagATune_01.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "5JArhMFjjejR",
    "outputId": "3c963b4e-9d83-49e7-c0c2-c7f3dd97ec5e"
   },
   "outputs": [],
   "source": [
    "data_m, time_sec_v = F_get_audio_features(DIR + dataset_subDIR + dataset_l[900]['mp3_path'], data_ext);\n",
    "plt.imshow(data_m, aspect='auto'); plt.colorbar()\n",
    "print('shape: {}, min: {}, max: {}'.format(data_m.shape, np.min(data_m), np.max(data_m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPYruf6jejS"
   },
   "source": [
    "# Patches\n",
    "\n",
    "When we deal with audio files, the audio representations/features is a matrix $W$ which represents the values of the representation/feature (such as spectrum, CQT, MFCC, Chroma, Log-Mel-gram) for each time frame of the audio file.\n",
    "\n",
    "If the analysis is performed with a 20ms hop size, and the total duration of the file is 30s; we therefore have a 1500 vectors; each vector represent the audio representation/feature at a specific time.\n",
    "\n",
    "The matrix $W$ has the size ```(nb_feature, nb_time_frame)```.\n",
    "\n",
    "To process it with a Convolutional Neural Network, we will split this big matrix into a set of small matrices $A_m$ (we name those ```patches```) where each $A_m$ represent a temporal slice of the big-matrix. \n",
    "The size of $A_m$ will be ```(nb_feature, size_of_patch)```.\n",
    "$W$ will then be represented by a set of $A_m$. \n",
    "Each $A_m$ has the same size but represent a slice of the big-matrix starting at a different time $t$.\n",
    "\n",
    "The two parameters we need to define are:\n",
    "- the distance in frames between two successive patch ```patch_hop_frame```\n",
    "- the width of the patch ```patch_halfduration_frame```\n",
    "\n",
    "Given an audio file, the following function compute the start and end position of each patch $A_m$ within $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0m2FPAZjejS"
   },
   "outputs": [],
   "source": [
    "def F_slice_into_patches(patch_hop_frame, patch_halfduration_frame, patch_info_l, nb_frame, idx_file, label):\n",
    "    \"\"\"\n",
    "    create structure for storing patch-based slides of the spectrogram\n",
    "    \"\"\"\n",
    "    middle_frame = patch_halfduration_frame\n",
    "    while middle_frame + patch_halfduration_frame < nb_frame:\n",
    "        if do_student:\n",
    "            # --- START CODE HERE\n",
    "            ...\n",
    "            # --- STOP CODE HERE\n",
    "\n",
    "        patch_info_d = {'idx_file': idx_file, \n",
    "                        'start_frame': start_frame, \n",
    "                        'middle_frame': middle_frame, \n",
    "                        'end_frame': stop_frame, \n",
    "                        'target': label}\n",
    "        patch_info_l.append(patch_info_d)\n",
    "\n",
    "        middle_frame += patch_hop_frame\n",
    "        return patch_info_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxV5e4MseBmx"
   },
   "source": [
    "### Test\n",
    "\n",
    "You should get the following output.\n",
    "\n",
    "```\n",
    "[{'end_frame': 64,\n",
    "  'idx_file': 333,\n",
    "  'middle_frame': 32,\n",
    "  'start_frame': 0,\n",
    "  'target': [1, 0, 0, 1, 0]},\n",
    " {'end_frame': 80,\n",
    "  'idx_file': 333,\n",
    "  'middle_frame': 48,\n",
    "  'start_frame': 16,\n",
    "  'target': [1, 0, 0, 1, 0]},\n",
    " {'end_frame': 96,\n",
    "  'idx_file': 333,\n",
    "  'middle_frame': 64,\n",
    "  'start_frame': 32,\n",
    "  'target': [1, 0, 0, 1, 0]}]\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIrRLXogjejS",
    "outputId": "ec91702f-ac38-41ef-f6a0-5353c93e1eef"
   },
   "outputs": [],
   "source": [
    "patch_info_l = F_slice_into_patches(patch_hop_frame=16, patch_halfduration_frame=32, patch_info_l=[], nb_frame=100, idx_file=333, label=[1,0,0, 1, 0])\n",
    "pp.pprint(patch_info_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QYDUdvYjejT"
   },
   "source": [
    "# Torch dataset\n",
    "During the training of our network, we would like to have batches of data $x^{(i)}, y^{(i)}$ on the fly, without having to take care about where to read it from, how to format it and to check that all data have been processed before moving to the next epoch.\n",
    "\n",
    "Pytorch provides a nice class for this: ``Dataset``.\n",
    "This class has three mandatory methods:\n",
    "- ``__init__``\n",
    "- ``__len__``\n",
    "- ``__getitem__``\n",
    "\n",
    "Let's explain this from the end.\n",
    "We instantiate the class using ``train_set = AudioDataSet(...)``.\n",
    "We define one item of our dataset as one patch.\n",
    "- Therefore ``len(train_set)`` should gives the number of patches of our dataset (which is the number of music tracks multiplied by the number of patches within a track). This is the goal of the ``__len__`` method.\n",
    "- ``train_set[idx_patch]`` should give all the information for a given patch, i.e. $(x^{(i)}, y^{(i)}$ for this patch as well as the audio file and time position it is coming from. This is the goal of the ``__getitem__`` method.\n",
    "- This means that ``__init__`` (the creator of the class) should store all the necessary information to allow ``__len__`` and ``__getitem__``.\n",
    "\n",
    "Note that, in order to allow runing the model faster, we will store a part of the information directly on the GPU. We do so using ``.cuda()``.\n",
    "\n",
    "We also add an option to the class to allow defining a dataset for the training and a dataset for the testing (here we do not split validation and testing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-MudEsPjejT"
   },
   "outputs": [],
   "source": [
    "class AudioDataSet(Dataset):\n",
    "    \"\"\"AudioDataSet Dataset\n",
    "    Args:\n",
    "        train (bool, optional)\n",
    "        audio_filename_l: list of audio files\n",
    "        labelname_l: list of label names\n",
    "    Attributes:\n",
    "        - audio_filename_l: list of audio files\n",
    "        - labelname_dict_l: list of the terms used for the labels\n",
    "        - patch_info_l: list of dictionaries containing patches information\n",
    "            'idx_file': identifier of the file used for this patch (reference to audio_filename_l)\n",
    "            'start_frame': starting frame of the patch in the audio file\n",
    "            'middle_frame': middle frame of the patch in the audio file\n",
    "            'end_frame': end frame of the patch in the audio file\n",
    "            'target': identifier of the target used for this patch (reference to labelname_dict_l)\n",
    "    \"\"\"\n",
    "\n",
    "    audio_filename_l = []\n",
    "    labelname_dict_l = []\n",
    "    patch_info_l = []\n",
    "    data_d = {}\n",
    "\n",
    "    def __init__(self, train, dataset_l, labelname_dict_l):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.labelname_dict_l = labelname_dict_l\n",
    "        \n",
    "        # --- split into train and test\n",
    "        if train: dataset_l = [dataset_l[idx] for idx in range(len(dataset_l)) if idx % 10 != 0]\n",
    "        else:     dataset_l = [dataset_l[idx] for idx in range(len(dataset_l)) if idx % 10 == 0]\n",
    "        \n",
    "        self.audio_filename_l = [DIR + dataset_subDIR + data['mp3_path'] for data in dataset_l]\n",
    "        \n",
    "        self.patch_info_l = []\n",
    "        self.data_d = {}\n",
    "        \n",
    "        if do_student:\n",
    "            # --- START CODE HERE\n",
    "            for idx_file in tqdm(range(len(self.audio_filename_l))):\n",
    "                ...\n",
    "                self.patch_info_l = F_slice_into_patches(patch_hop_frame, patch_halfduration_frame, self.patch_info_l, nb_frame, idx_file, label)\n",
    "                self.data_d[self.audio_filename_l[idx_file]] = {'data_m': torch.from_numpy(data_m).float().cuda(), 'time_sec_v': time_sec_v}\n",
    "            # --- STOP CODE HERE\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return len(self.patch_info_l)\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx_patch):\n",
    "        \"\"\"\n",
    "        return sample['idx_file','middle_frame','target','data']\n",
    "        \"\"\"\n",
    "        audio_filename = self.audio_filename_l[ self.patch_info_l[idx_patch]['idx_file'] ]\n",
    "        # --- Optimize: get directly from GPU\n",
    "        data_m, time_sec_v = self.data_d[audio_filename]['data_m'], self.data_d[audio_filename]['time_sec_v']\n",
    "        STEP_sec = np.mean(np.diff(time_sec_v))\n",
    "        # --- get the patch\n",
    "        data_m = data_m[:, self.patch_info_l[idx_patch]['start_frame']:self.patch_info_l[idx_patch]['end_frame']]\n",
    "        \n",
    "        # --- create output structure\n",
    "        sample = {'idx_file': self.patch_info_l[idx_patch]['idx_file'],\n",
    "                  'middle_frame': self.patch_info_l[idx_patch]['middle_frame'],\n",
    "                  'target': self.patch_info_l[idx_patch]['target'],\n",
    "                  'data': data_m,\n",
    "                  'STEP_sec': STEP_sec\n",
    "                  }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "th_XfjxM5Vtx"
   },
   "source": [
    "Using this class, we instantiate a ``train_set`` and a ``test_set``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4JgSU6jjejT",
    "outputId": "85ab80a9-7061-4905-fc30-8d17988a21c0"
   },
   "outputs": [],
   "source": [
    "train_set = AudioDataSet(train=True, dataset_l=dataset_l, labelname_dict_l=labelname_dict_l)\n",
    "test_set = AudioDataSet(train=False, dataset_l=dataset_l, labelname_dict_l=labelname_dict_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exnvT5CnfGZL"
   },
   "source": [
    "### Test\n",
    "\n",
    "You should get the following output.\n",
    "\n",
    "```\n",
    "len(train_set):  31490\n",
    "len(test_set):  3500\n",
    "/content/drive//My Drive/_sound//_gtzan//dataset_gtzan//pop/pop.00065.wav\n",
    "torch.Size([128, 68])\n",
    "0.023219954648526078\n",
    "duration of a patch in second: 1.5789569160997732\n",
    "```\n",
    "\n",
    "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_MagnaTagATune_02.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "dzlAAK6RjejU",
    "outputId": "69cda099-2780-4185-c7f3-0ef41552c7ac"
   },
   "outputs": [],
   "source": [
    "print('len(train_set): ', len(train_set))\n",
    "print('len(test_set): ', len(test_set))\n",
    "\n",
    "idx_file = 50\n",
    "print(train_set.audio_filename_l[idx_file])\n",
    "print(train_set[idx_file]['data'].size())\n",
    "print(train_set[idx_file]['STEP_sec'])\n",
    "plt.imshow(train_set[idx_file]['data'].cpu().numpy()); plt.colorbar();\n",
    "\n",
    "print('duration of a patch in second:', train_set[idx_file]['STEP_sec']*train_set[idx_file]['data'].size(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CdS5uMyo55lR"
   },
   "source": [
    "# Torch dataloader\n",
    "\n",
    "```DataLoader```is a **very very convenient** way pytorch to provide to load on the fly the data necessary for a given mini-batch. It will simply pick-up from our DataSets the data necessary for the given mini-batch. We do not need to care about counting them or shuffling them. It does everything for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jla0kEjGjejU"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Z21KIiI6Nkz"
   },
   "source": [
    "### Test\n",
    "\n",
    "You should get the following output.\n",
    "\n",
    "```\n",
    "torch.Size([32, 128, 68])\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHcTj3-OjejU",
    "outputId": "55f7956c-8fe8-4f53-d660-1985e48c3d28"
   },
   "outputs": [],
   "source": [
    "one_mini_batch = next(iter(train_loader))\n",
    "one_mini_batch['data'].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xKPGMLWjejU"
   },
   "source": [
    "# DNN model\n",
    "\n",
    "We now define the architecture of our Deep ConvNet to perform multi-label classification.\n",
    "The network has the following architecture.\n",
    "\n",
    "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_MagnaTagATune_03.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9OgbcoWjejV"
   },
   "outputs": [],
   "source": [
    "class DeepConvNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_in_H, nb_class):\n",
    "        \"\"\"\n",
    "        inputs:\n",
    "        - n_in_H: input dimension (128 in the case of Log-Mel-Gram)\n",
    "        - nb_class: output dimension (number of tags to be estimated)\n",
    "        \"\"\"\n",
    "\n",
    "        super(DeepConvNet, self).__init__()\n",
    "        \n",
    "        if do_student:\n",
    "            # --- START CODE HERE\n",
    "            ...\n",
    "            # --- STOP CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if do_student:\n",
    "            # --- START CODE HERE\n",
    "            ...\n",
    "            # --- STOP CODE HERE\n",
    "\n",
    "  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfLWPxWP_o-a"
   },
   "source": [
    "### Test\n",
    "\n",
    "We instantiate the model, send it to the GPU and display its summary.\n",
    "You should get the following output.\n",
    "\n",
    "```\n",
    "n_in_H:  128\n",
    "cuda:0\n",
    "torch.Size([2, 1, 128, 68])\n",
    "l1_conv:  torch.Size([2, 256, 1, 64])\n",
    "l1_pool:  torch.Size([2, 256, 1, 16])\n",
    "l2_conv:  torch.Size([2, 256, 1, 12])\n",
    "l2_pool: torch.Size([2, 256, 1, 6])\n",
    "l3_conv:  torch.Size([2, 256, 1, 2])\n",
    "l3_conv:  torch.Size([2, 256, 1, 1])\n",
    "l4_fc:  torch.Size([2, 256])\n",
    "l5_fc:  torch.Size([2, 256])\n",
    "l6_fc:  torch.Size([2, 12])\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Conv2d-1           [-1, 256, 1, 64]         164,096\n",
    "             PReLU-2           [-1, 256, 1, 64]               1\n",
    "         MaxPool2d-3           [-1, 256, 1, 16]               0\n",
    "           Dropout-4           [-1, 256, 1, 16]               0\n",
    "            Conv2d-5           [-1, 256, 1, 12]         327,936\n",
    "             PReLU-6           [-1, 256, 1, 12]               1\n",
    "         MaxPool2d-7            [-1, 256, 1, 6]               0\n",
    "           Dropout-8            [-1, 256, 1, 6]               0\n",
    "            Conv2d-9            [-1, 256, 1, 2]         327,936\n",
    "            PReLU-10            [-1, 256, 1, 2]               1\n",
    "        MaxPool2d-11            [-1, 256, 1, 1]               0\n",
    "          Dropout-12                  [-1, 256]               0\n",
    "           Linear-13                  [-1, 256]          65,792\n",
    "            PReLU-14                  [-1, 256]               1\n",
    "          Dropout-15                  [-1, 256]               0\n",
    "           Linear-16                  [-1, 256]          65,792\n",
    "            PReLU-17                  [-1, 256]               1\n",
    "          Dropout-18                  [-1, 256]               0\n",
    "           Linear-19                   [-1, 12]           3,084\n",
    "================================================================\n",
    "Total params: 954,641\n",
    "Trainable params: 954,641\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.03\n",
    "Forward/backward pass size (MB): 0.41\n",
    "Params size (MB): 3.64\n",
    "Estimated Total Size (MB): 4.08\n",
    "----------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__SddXs8jejV",
    "outputId": "2abe8a96-7f9c-40f6-ab6e-60a7e11b95c6"
   },
   "outputs": [],
   "source": [
    "# --- Instantiate the model\n",
    "one_mini_batch = next(iter(train_loader))\n",
    "n_in_H = one_mini_batch['data'].size(1)\n",
    "print('n_in_H: ', n_in_H)\n",
    "model = DeepConvNet(n_in_H=n_in_H, nb_class=len(labelname_dict_l))\n",
    "\n",
    "# --- Send the model to GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.cuda()\n",
    "\n",
    "# --- Display the structure of the model\n",
    "from torchsummary import summary\n",
    "summary(model, input_size=(1, n_in_H, 68))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP971QDpjejW"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "To train the model, we still need to define a loss to be minimized\n",
    "- what is the loss for a multi-label problem ? \n",
    "- how to gives more weight to the positive classes ?\n",
    "\n",
    "We also need to define an optimizer. We will use Adam with a learning rate of 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFw9SCazjejW"
   },
   "outputs": [],
   "source": [
    "if do_student:\n",
    "    # --- START CODE HERE\n",
    "    criterion = ...\n",
    "    optimizer = ...\n",
    "    # --- STOP CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CILd1qSokQzp"
   },
   "source": [
    "### Test\n",
    "\n",
    "Before launching the training, we test the whole pipeline on one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhXdL6e4kP5Q"
   },
   "outputs": [],
   "source": [
    "one_mini_batch = next(iter(train_loader))\n",
    "x = one_mini_batch['data'][:,None,:,:]\n",
    "y = one_mini_batch['target'].cuda()\n",
    "y_hat = model(x)\n",
    "loss = criterion(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TATthajQiOJ3"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(DIR)\n",
    "import tools_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2cCDEF1jejW"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, num_epoch):\n",
    "    \"\"\"\n",
    "    train the model\n",
    "    \"\"\"\n",
    "    # --- set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    my_score = tools_score.Score(do_class_method)\n",
    "\n",
    "    # --- loop over mini-batches\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "        # --- set gradient to zero (in order to avoid cumulating them over batch)\n",
    "        model.zero_grad()        \n",
    "        # --- we need to add the channel dimension to the input\n",
    "        x = batch['data'][:,None,:,:]        \n",
    "        y = batch['target'].cuda()\n",
    "        y_hat = model(x)        \n",
    "        loss = criterion(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()                \n",
    "        \n",
    "        my_score.add(loss, y_hat, y)\n",
    "           \n",
    "    my_score.print('Train', num_epoch)\n",
    "    return my_score.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IS8qHFk4jejW"
   },
   "outputs": [],
   "source": [
    "def test(test_loader, num_epoch):\n",
    "    \"\"\"\n",
    "    test the model\n",
    "    \"\"\"\n",
    "    # --- set model to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    my_score = tools_score.Score(do_class_method)\n",
    "\n",
    "    # --- loop over mini-batches\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        # --- We need to add the channel dimension to the input\n",
    "        x = batch['data'][:,None,:,:]\n",
    "        y = batch['target'].cuda()\n",
    "        y_hat = model(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "\n",
    "        my_score.add(loss, y_hat, y)\n",
    "    \n",
    "    my_score.print('Test', num_epoch) \n",
    "    return my_score.get_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZULp5xyMwOB4"
   },
   "source": [
    "#  Do training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2VrmS706jejW",
    "outputId": "038c7fe8-dbec-4ef7-eab3-fcd6451887af"
   },
   "outputs": [],
   "source": [
    "if do_model_train==True:\n",
    "    score_train_l, score_test_l = [], []\n",
    "    # --- loop over epochs\n",
    "    for num_epoch in range(nb_epoch):    \n",
    "        score_train = train(train_loader, num_epoch)\n",
    "        score_test = test(test_loader, num_epoch)\n",
    "        score_train_l.append(score_train)\n",
    "        score_test_l.append(score_test)\n",
    "    torch.save(model.state_dict(), DIR + 'model_torch' + data_ext)\n",
    "else:\n",
    "    model.load_state_dict(torch.load(DIR + 'model_torch' + data_ext))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-ODoNSRQEJY"
   },
   "source": [
    "## Display training/test curves\n",
    "\n",
    "We now display the loss, accuracy, TPrate, TNrate for the training and test set.\n",
    "You should get the following output.\n",
    "\n",
    "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_MagnaTagATune_04.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "5L0SJQ5HyI2_",
    "outputId": "6fe244d4-3dcb-48a3-975f-87c0c5b8be1e"
   },
   "outputs": [],
   "source": [
    "# --- Display loss and accuracy for train and test set\n",
    "my_score = tools_score.Score(do_class_method)\n",
    "my_score.plot_curve(score_train_l, score_test_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS_u2edQjejX"
   },
   "source": [
    "# Display results as tag-o-gram\n",
    "\n",
    "Now that our model has been trained we will use it to create a tag-o-gram (tag likelihood over the patches of an audio file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWcYFyOqjejX"
   },
   "outputs": [],
   "source": [
    "def F_tag_o_gram(audio_file):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if do_student:\n",
    "        # --- START CODE HERE\n",
    "        ...\n",
    "        # --- STOP CODE HERE\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(16, 6))\n",
    "    im = axes.imshow(y_hat_m.T, aspect='auto', cmap=plt.get_cmap('inferno'))\n",
    "    axes.set_yticks(np.arange(0,len(test_set.labelname_dict_l)))\n",
    "    axes.set_yticklabels(test_set.labelname_dict_l)\n",
    "    axes.grid(True)\n",
    "    fig.colorbar(im, orientation='vertical')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYN2GjWCj3nA"
   },
   "source": [
    "### Test\n",
    "\n",
    "We now test our tag-o-gram on an audio file.\n",
    "You should get the following output.\n",
    "\n",
    "```\n",
    "{'album': 'CD2 The Sowebo Concert',\n",
    " 'artist': 'Jay Kishor',\n",
    " 'clip_id': 3649,\n",
    " 'mp3_path': 'c/jay_kishor-cd2_the_sowebo_concert-01-raga_malkauns-146-175.mp3',\n",
    " 'original_url': 'http://he3.magnatune.com/all/01-Raga%20Malkauns-Jay%20Kishor.mp3',\n",
    " 'segmentEnd': 175,\n",
    " 'segmentStart': 146,\n",
    " 'tag': ['guitar', 'quiet', 'india'],\n",
    " 'target': array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
    " 'title': 'Raga Malkauns',\n",
    " 'track_number': 1,\n",
    " 'url': 'http://www.magnatune.com/artists/albums/kishor-sowebo2/'}\n",
    " ````\n",
    "\n",
    "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_MagnaTagATune_05.png\">\n",
    "\n",
    "As you see, while the track has been tagged 'guitar', it actually contains a [sitar](https://en.wikipedia.org/wiki/Sitar); which has been correctly detected by our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "VOSb7a2lDHU1",
    "outputId": "b966c749-e5d1-49f1-8c57-70414cd87f2d"
   },
   "outputs": [],
   "source": [
    "idx_file = 902\n",
    "test_file = DIR + dataset_subDIR + dataset_l[idx_file]['mp3_path']\n",
    "F_tag_o_gram(audio_file = test_file)\n",
    "pp.pprint(dataset_l[idx_file])\n",
    "IPython.display.Audio(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSJTkNM-jejX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Lab_Audio_MagnaTagATune_#pytorch_#2022-02-03_#teacher.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
